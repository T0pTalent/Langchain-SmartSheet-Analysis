{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GkSkcSIU0DSu",
        "yQJgkE0vzzmb",
        "VDlo3EcJz4vW",
        "wIx02G1Ez7jX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install smartsheet-python-sdk langchain pandas pinecone-client gradio"
      ],
      "metadata": {
        "id": "M50nOI5vyLkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TKgkPnRKx_Gl"
      },
      "outputs": [],
      "source": [
        "import smartsheet\n",
        "import re\n",
        "import pandas\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "import pinecone\n",
        "from datetime import date\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pinecone Config"
      ],
      "metadata": {
        "id": "GkSkcSIU0DSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = \"488f39e6-74bd-40e7-ad4e-2a48c66590f0\"\n",
        "PINECONE_ENV = \"us-east4-gcp\""
      ],
      "metadata": {
        "id": "H9TCurEmyGOg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Knowledge Base"
      ],
      "metadata": {
        "id": "yQJgkE0vzzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_phase_number(text):\n",
        "    # pattern = r\"phase\\s*\\d\"\n",
        "    pattern = r\"phase\\s\"\n",
        "\n",
        "    return bool(re.search(pattern, text.lower()))"
      ],
      "metadata": {
        "id": "-l6ULC4XylYw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_all_conent(access_token, sheet_id):\n",
        "    smart_client = smartsheet.Smartsheet(access_token=access_token)\n",
        "    current_sheet = smart_client.Sheets.get_sheet(sheet_id)\n",
        "    columns = smart_client.Sheets.get_columns(sheet_id).data\n",
        "\n",
        "    headers = [c.title for c in columns]\n",
        "\n",
        "    column_ids = [[c for c in columns if c.title == h][0].id for h in headers]\n",
        "\n",
        "    raw_text = \"\"\n",
        "    temp_text = \"\"\n",
        "    phase_id = 1\n",
        "\n",
        "    phases = []\n",
        "    individual_phases = []\n",
        "\n",
        "    pre_available = False\n",
        "\n",
        "    for row in current_sheet.rows:\n",
        "        values = [\n",
        "            [cell.value for cell in row.cells if cell.column_id == col][0]\n",
        "            for col in column_ids\n",
        "        ]\n",
        "        if values[headers.index('Task Name')] is not None and contains_phase_number(values[headers.index('Task Name')]):\n",
        "            raw_text += '\\n\\n' + '-' * 30 + f' {phase_id}. ' + values[headers.index('Task Name')] + ' ' + '-'*30 +'\\n'\n",
        "            phases.append(f' {phase_id}. ' + values[headers.index('Task Name')])\n",
        "            phase_id += 1\n",
        "\n",
        "            individual_phases.append(temp_text)\n",
        "            temp_text = '\\n\\n' + '-' * 30 + ' ' + values[headers.index('Task Name')] + ' ' + '-'*30 +'\\n'\n",
        "        for i in range(len(values)):\n",
        "            if values[i] != None:\n",
        "                raw_text += headers[i] + \": \" + str(values[i]) + \"\\n\"\n",
        "                temp_text += headers[i] + \": \" + str(values[i]) + \"\\n\"\n",
        "        raw_text += \"\\n\"\n",
        "        temp_text += '\\n'\n",
        "\n",
        "    individual_phases.append(temp_text)\n",
        "\n",
        "    with open(\"processed_files/phases.txt\", \"w\") as phase_file:\n",
        "        phase_file.write('\\n'.join(phases))\n",
        "\n",
        "    with open(\"processed_files/all_content.txt\", \"w\") as raw_file:\n",
        "        raw_file.write(raw_text)\n",
        "\n",
        "    if individual_phases[0] is not None and len(individual_phases[0]) > 1000:\n",
        "        pre_available = True\n",
        "        with open(\"processed_files/pre.txt\", \"w\") as pre_file:\n",
        "            pre_file.write(individual_phases[0])\n",
        "\n",
        "    for i in range(1, len(individual_phases)):\n",
        "        with open(f\"processed_files/phase{i}.txt\",\"w\") as file:\n",
        "            file.write(individual_phases[i])\n",
        "\n",
        "    print('Preprocess Done!!!')\n",
        "\n",
        "    return pre_available"
      ],
      "metadata": {
        "id": "3B5PRarHyoNV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_index():\n",
        "    for index in pinecone.list_indexes():\n",
        "        pinecone.delete_index(index)\n",
        "\n",
        "    print('Deleted All Indexes')"
      ],
      "metadata": {
        "id": "8iIGoIwGzDaw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_pinecone_index(filename, index_name, chunck_size, openai_key):\n",
        "    loader = TextLoader(f\"processed_files/{filename}.txt\")\n",
        "    documents = loader.load()\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=chunck_size, chunk_overlap=0)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "\n",
        "    if index_name not in pinecone.list_indexes():\n",
        "        pinecone.create_index(name=index_name, metric=\"dotproduct\", dimension=1536)\n",
        "        vectorstore = Pinecone.from_documents(\n",
        "            docs, embedding=embeddings, index_name=index_name\n",
        "        )\n",
        "\n",
        "        print('Created Index:', index_name)"
      ],
      "metadata": {
        "id": "XiFr4cz0zF-4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_knowledge_base(access_token, sheet_id, openai_key, model_name):\n",
        "    pre_available = preprocess_all_conent(access_token, sheet_id)\n",
        "\n",
        "    pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
        "\n",
        "    delete_index()\n",
        "\n",
        "    build_pinecone_index(\"all_content\", 'all', 450, openai_key)\n",
        "\n",
        "    if pre_available:\n",
        "        build_pinecone_index('pre', 'baseline', 450, openai_key)\n",
        "\n",
        "    phases = [s.strip() for s in open('processed_files/phases.txt', 'r').readlines()]\n",
        "\n",
        "    for i in range(len(phases)):\n",
        "        build_pinecone_index(f'phase{i+1}', f'phase{i+1}', 450, openai_key)\n",
        "\n",
        "    if pre_available:\n",
        "        phases = ['all', 'baseline'] + phases\n",
        "    else:\n",
        "        phases = ['all'] + phases\n",
        "\n",
        "    with open(\"processed_files/phases.txt\", \"w\") as phase_file:\n",
        "        phase_file.write('\\n'.join(phases))\n",
        "\n",
        "    return phases"
      ],
      "metadata": {
        "id": "Dut1rQVByq-t"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smartsheet Analysis"
      ],
      "metadata": {
        "id": "VDlo3EcJz4vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)"
      ],
      "metadata": {
        "id": "lT0EZLQkytFe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_analysis(openai_key, model_name, phase):\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "\n",
        "    pinecone_index = pinecone.Index(index_name=\"all\")\n",
        "    vectorstore = Pinecone(\n",
        "        index=pinecone_index, embedding=embeddings, text_key=\"text\"\n",
        "    )\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        openai_api_key=openai_key, temperature=0, model_name=model_name\n",
        "    )\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=model, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "\n",
        "    today = date.today()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    % Who you are:\n",
        "    - You are a professional excellent Project Manager.\n",
        "\n",
        "    % What you should know:\n",
        "    - What you know is all about the one project.\n",
        "    - Today is {today}.\n",
        "    - Mustn't consider weekends when calculating date. So You MUST KNOW that a week has 5 workdays when calculating date.\n",
        "\n",
        "    % What you do:\n",
        "    - Write your detailed analysis about {phase}.\n",
        "    - Write current progress of the phase: what to do in the phase, what have been done and what should do in the phase\n",
        "    - Write what the observation of the project is.\n",
        "    - Discover potential risk\n",
        "    - Extract insight\n",
        "    - Recommendation for future\n",
        "    \"\"\"\n",
        "\n",
        "    print(phase)\n",
        "\n",
        "    response = chain.run(prompt)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "UOHwKNAOzPT2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(openai_key, model_name, phase, query):\n",
        "    phase_id = phase.split('.')[0]\n",
        "\n",
        "    index_name = 'phase' + phase_id if phase_id.isdigit() else phase\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "\n",
        "    pinecone_index = pinecone.Index(index_name=index_name)\n",
        "    vectorstore = Pinecone(\n",
        "        index=pinecone_index, embedding=embeddings, text_key=\"text\"\n",
        "    )\n",
        "    model = ChatOpenAI(\n",
        "        openai_api_key=openai_key, temperature=0, model_name=model_name\n",
        "    )\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=model, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "\n",
        "    today = date.today()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a professional project manager.\n",
        "    Answer the following question as best and detailed as possible.\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    response = chain.run(prompt)\n",
        "\n",
        "    print(\"Query: \", query)\n",
        "    print('Answer: ', response)\n",
        "    return response"
      ],
      "metadata": {
        "id": "o6RxaRZzzR-W"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Interface"
      ],
      "metadata": {
        "id": "wIx02G1Ez7jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as pre_ui:\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # Build Knowledge Base\n",
        "            \"\"\"\n",
        "        )\n",
        "        with gr.Row():\n",
        "            smart_access_token = gr.Textbox(label=\"SmartSheet Access Token\")\n",
        "            sheet_id = gr.Textbox(label=\"Sheet ID\")\n",
        "        with gr.Row():\n",
        "            _opeanai_key = gr.Textbox(show_label=False, placeholder=\"OpenAI API Key\")\n",
        "            _model_name = gr.Dropdown(\n",
        "                [\n",
        "                    \"gpt-4-0314\",\n",
        "                    \"gpt-4-0613\",\n",
        "                    \"gpt-4\",\n",
        "                    \"gpt-3.5-turbo-0613\",\n",
        "                    \"gpt-3.5-turbo-0301\",\n",
        "                    \"gpt-3.5-turbo-16k-0613\",\n",
        "                ],\n",
        "                show_label=False,\n",
        "            )\n",
        "        with gr.Row():\n",
        "            result = gr.Textbox(label=\"Result\")\n",
        "            b_build = gr.Button(\"Build\")\n",
        "\n",
        "    def build_knowledge_base(access_token, sheet_id, openai_key, model_name):\n",
        "        phases = build_knowledge_base(access_token, sheet_id, openai_key, model_name)\n",
        "        _phases = phases[2: ] if phases[1] == 'baseline' else phases[1:]\n",
        "        return 'Success!', gr.Dropdown(_phases), gr.Dropdown(phases)\n",
        "\n",
        "\n",
        "with gr.Blocks() as run_ui:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Project Schedule Analysis\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        opeanai_key = gr.Textbox(show_label=False, placeholder=\"OpenAI API Key\")\n",
        "        model_name = gr.Dropdown(\n",
        "            [\n",
        "                \"gpt-4-0314\",\n",
        "                \"gpt-4-0613\",\n",
        "                \"gpt-4\",\n",
        "                \"gpt-3.5-turbo-0613\",\n",
        "                \"gpt-3.5-turbo-0301\",\n",
        "                \"gpt-3.5-turbo-16k-0613\",\n",
        "            ],\n",
        "            show_label=False,\n",
        "        )\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            analysis_text = gr.TextArea(label=\"Detailed Analysis\", lines=15, max_lines=15)\n",
        "            with gr.Row():\n",
        "                choose_phase = gr.Dropdown([])\n",
        "                b_phase = gr.Button(\"Get Analysis\")\n",
        "        with gr.Column():\n",
        "            choice = gr.Dropdown(\n",
        "                [],\n",
        "                label=\"Pinecone Index\",\n",
        "            )\n",
        "            query = gr.Textbox(label=\"Query:\")\n",
        "            banswer = gr.Button(\"Generate Answer\")\n",
        "            answer = gr.TextArea(label=\"Answer:\", lines=6, max_lines=10)\n",
        "\n",
        "\n",
        "    def set_analysis(opeanai_key, model_name, phase_text):\n",
        "        response = generate_analysis(\n",
        "            opeanai_key, model_name, phase_text\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    def set_answer(opeanai_key, model_name, index_name, query):\n",
        "        if index_name == \"\" or query == \"\":\n",
        "            return \"Choose Index and Input QUERY!!!\"\n",
        "        response = generate_answer(\n",
        "            opeanai_key, model_name, index_name, query\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    b_build.click(build_knowledge_base, inputs=[smart_access_token, sheet_id, _opeanai_key, _model_name], outputs=[result, choose_phase, choice])\n",
        "\n",
        "    b_phase.click(set_analysis, [opeanai_key, model_name, choose_phase], outputs=analysis_text)\n",
        "    banswer.click(set_answer, [opeanai_key, model_name, choice, query], outputs=answer)\n",
        "\n",
        "\n",
        "demo = gr.TabbedInterface([pre_ui, run_ui], [\"Build Knowledge-Base\", \"Run System\"])\n",
        "demo.queue().launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "QExW5qgXzUH2",
        "outputId": "0ff5e7b0-59a1-4f0c-89eb-3e8cc2c3b3fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://67cd927db5c843c147.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://67cd927db5c843c147.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5LKyJFuzZb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}